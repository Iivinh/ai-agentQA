# -*- coding: utf-8 -*-
"""
PDF ‚Üí (one-shot per file) plan b·∫±ng anchors c√¢u (m·ªãn) ‚Üí materialize ‚Üí auto-refine ‚Üí embed ‚Üí index (Elasticsearch)
- KH√îNG d√πng ENV, KH√îNG d√πng argparse.
- √âP h·∫°t m·ªãn b·∫±ng r√†ng bu·ªôc ƒë·ªô d√†i + h·∫≠u ki·ªÉm t·ª± chia nh·ªè.

Y√™u c·∫ßu:
  pip install pdfplumber google-generativeai tqdm elasticsearch==8.* regex
"""

import os, json, time, random
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

import pdfplumber
from tqdm import tqdm
from elasticsearch import Elasticsearch, helpers
import google.generativeai as genai
import regex as re

# ===================== C·∫§U H√åNH TR·ª∞C TI·∫æP =====================
PDF_DIR        = "data/pdf"
ES_URL         = "http://localhost:9200"
ES_INDEX       = "school_knowledge"
GEMINI_API_KEY = "AIzaSyAJlOwMZm7n08S-vqfzgISw1P-0D-UcnlI"
EMBED_MODEL    = "models/gemini-embedding-001"
SEGMENT_MODEL  = "gemini-2.5-pro"
PLAN_BATCH_PAGES = 10 
# L∆∞u ki·ªÉm tra
SAVE_CLEAN     = True
SAVE_PLAN      = True
SAVE_PLAN_RAW  = False
SAVE_SEGMENT   = True
USE_EXISTING_PLAN = True

# Ki·ªÉm so√°t ƒë·ªô h·∫°t
ANCHOR_MAX_CHARS   = 120
TARGET_SEG_CHARS   = 2000     # m·ª•c ti√™u  ~900 k√Ω t·ª±/segment
MIN_SEG_CHARS      = 0     # ƒë·ª´ng ƒë·ªÉ qu√° ng·∫Øn (tr·ª´ ti√™u ƒë·ªÅ)
MAX_SEG_CHARS      = 3000    # tr·∫ßn tuy·ªát ƒë·ªëi; > tr·∫ßn s·∫Ω b·ªã t·ª± chia nh·ªè
MAX_SPANS_PER_SEG  = 2       # m·ªói segment b·∫Øc t·ªëi ƒëa qua 2 trang (n·∫øu d√†i h∆°n ‚Üí LLM ph·∫£i chia nh·ªè)
SNAP_TO_BOUNDARY   = True
MIN_CHUNK_CHARS    = 8

# ===================== K·∫æT N·ªêI & KI·ªÇM TRA =====================
def get_es() -> Elasticsearch:
    return Elasticsearch(ES_URL)

def configure_gemini():
    if not GEMINI_API_KEY or GEMINI_API_KEY == "PUT_YOUR_GEMINI_API_KEY_HERE":
        raise RuntimeError("Ch∆∞a c·∫•u h√¨nh GEMINI_API_KEY ·ªü ƒë·∫ßu file.")
    genai.configure(api_key=GEMINI_API_KEY, transport="rest")

def check_connections():
    print("üîå Ki·ªÉm tra Elasticsearch‚Ä¶")
    try:
        es = get_es()
        ok = es.ping()
        info = es.info() if ok else {}
        print(f"  ‚Ä¢ ES_URL: {ES_URL} | Ping: {'OK' if ok else 'FAIL'}")
        if ok:
            print(f"  ‚Ä¢ Cluster: {info.get('cluster_name')} | Version: {info.get('version',{}).get('number')}")
            try:
                exists = es.indices.exists(index=ES_INDEX)
                print(f"  ‚Ä¢ Index '{ES_INDEX}': {'t·ªìn t·∫°i' if exists else 'ch∆∞a c√≥'}")
            except Exception as e:
                print(f"  ‚Ä¢ Kh√¥ng ki·ªÉm tra ƒë∆∞·ª£c index: {e}")
    except Exception as e:
        print(f"‚ùå L·ªói Elasticsearch: {e}")

    print("\nü§ñ Ki·ªÉm tra Gemini‚Ä¶")
    try:
        configure_gemini()
        r = genai.embed_content(model=EMBED_MODEL, content="health check")
        dims = len(r.get("embedding", []))
        print(f"  ‚Ä¢ EMBED_MODEL: {EMBED_MODEL} (dims={dims or 'UNKNOWN'})")
        txt = (genai.GenerativeModel(SEGMENT_MODEL)
               .generate_content("Ch·ªâ tr·∫£ l·ªùi: OK").text or "").strip()
        print(f"  ‚Ä¢ SEGMENT_MODEL: {SEGMENT_MODEL} | Gen sample: {txt[:40]}")
        print("‚úÖ Gemini OK")
    except Exception as e:
        print(f"‚ùå L·ªói Gemini: {e}")

# ===================== PDF CLEANING =====================
def clean_pdf_text(text: str) -> str:
    text = text or ""
    text = re.sub(r'(\w)-\s*\n(\w)', r'\1\2', text, flags=re.UNICODE)
    text = text.replace('\r', '')
    placeholder = "<<<PARA>>>"
    text = text.replace("\n\n", placeholder)
    text = re.sub(r'[ \t]*\n[ \t]*', ' ', text)
    text = text.replace(placeholder, "\n\n")
    text = re.sub(r'[ \t]+', ' ', text)
    return text.strip()

def extract_pdf_pages(pdf_path: str, min_len: int = 20) -> List[Dict[str, Any]]:
    pages: List[Dict[str, Any]] = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for i, p in enumerate(pdf.pages, start=1):
                raw = p.extract_text() or ""
                txt = clean_pdf_text(raw)
                if txt and len(txt) >= min_len:
                    pages.append({"page_num": i, "text": txt})
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc {os.path.basename(pdf_path)}: {e}")
    return pages

def read_and_clean_pdfs() -> Dict[str, List[Dict[str, Any]]]:
    if not os.path.isdir(PDF_DIR):
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c PDF kh√¥ng t·ªìn t·∫°i: {PDF_DIR}")
        return {}
    pdfs = [os.path.join(PDF_DIR, f) for f in os.listdir(PDF_DIR) if f.lower().endswith(".pdf")]
    print(f"üìÇ T√¨m th·∫•y {len(pdfs)} file PDF trong {PDF_DIR}")

    results: Dict[str, List[Dict[str, Any]]] = {}
    for pdf_path in pdfs:
        base = os.path.basename(pdf_path); doc_id = os.path.splitext(base)[0]
        pages = extract_pdf_pages(pdf_path, min_len=20)
        if not pages:
            print(f"‚ö†Ô∏è B·ªè qua (kh√¥ng c√≥ text h·ª£p l·ªá): {base}")
            continue
        results[doc_id] = pages
        print(f"‚úÖ {base}: {len(pages)} trang h·ª£p l·ªá")
        if SAVE_CLEAN:
            os.makedirs("clean", exist_ok=True)
            with open(os.path.join("clean", f"{doc_id}.pages.jsonl"), "w", encoding="utf-8") as f:
                for rec in pages: f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            with open(os.path.join("clean", f"{doc_id}.full.txt"), "w", encoding="utf-8") as f:
                for rec in pages: f.write(f"=== Page {rec['page_num']} ===\n{rec['text']}\n\n")
            print(f"   ‚Ü≥ ƒê√£ l∆∞u clean/{doc_id}.pages.jsonl & clean/{doc_id}.full.txt")
    return results

# ===================== JSON HELPERS =====================
def _strip_code_fences(s: str) -> str:
    s = s.strip()
    if s.startswith("```"):
        s = re.sub(r"^```(?:json|JSON)?\s*", "", s); s = re.sub(r"\s*```$", "", s)
    return s.strip()

def _largest_json_object(s: str) -> Optional[str]:
    start=None; depth=0; best=None
    for i,ch in enumerate(s):
        if ch=='{':
            if depth==0: start=i
            depth+=1
        elif ch=='}':
            if depth>0:
                depth-=1
                if depth==0 and start is not None:
                    cand=s[start:i+1]
                    if not best or len(cand)>len(best): best=cand
    return best

def _safe_json_loads(s: str) -> Optional[dict]:
    if not s: return None
    s=_strip_code_fences(s)
    try: return json.loads(s)
    except Exception: pass
    blob=_largest_json_object(s)
    if blob:
        try: return json.loads(blob)
        except Exception: return None
    return None

def _sanitize_for_log(s: str) -> str:
    return ''.join(ch for ch in s if ch=='\n' or 32<=ord(ch)<=126 or ord(ch)>=160).strip()

# ===================== ANCHOR & SNAP HELPERS =====================
_PUNCT = set(list(".,;:!?‚Ä¶‚Äú‚Äù\"'()[]{}<>‚Äî‚Äì-‚Ä¢¬∑/\\|"))
def _is_space(c: str) -> bool: return c.isspace()
def _is_punct(c: str) -> bool: return c in _PUNCT
def _is_alnum(c: str) -> bool: return c.isalnum()
def _at_boundary_left(t: str, i: int) -> bool: return i<=0 or _is_space(t[i-1]) or _is_punct(t[i-1])
def _at_boundary_right(t: str, i: int) -> bool: return i>=len(t) or _is_space(t[i]) or _is_punct(t[i])

def _snap_span_to_word_sentence(t: str, start: int, end: int) -> Tuple[int,int]:
    n=len(t); start=max(0,min(start,n)); end=max(start,min(end,n))
    if start<end:
        if start>0 and _is_alnum(t[start-1]) and start<n and _is_alnum(t[start]):
            if start-1>=0 and (_at_boundary_left(t,start-1) or _is_space(t[start-1]) or _is_punct(t[start-1])):
                start-=1
            else:
                while start>0 and _is_alnum(t[start-1]) and not _at_boundary_left(t,start): start-=1
        if end>0 and end<n and _is_alnum(t[end-1]) and _is_alnum(t[end]):
            if end+1<=n: end+=1
            while end<n and _is_alnum(t[end]) and not _at_boundary_right(t,end): end+=1
        while end<n and not _at_boundary_right(t,end) and not _is_space(t[end]) and not _is_punct(t[end]): end+=1
    return (start,end)

def _needs_joiner(prev_text: str) -> bool:
    if not prev_text: return False
    return not bool(re.search(r'[\s\.\!\?\:\;\,\)\]\}]+$', prev_text))

def _normalize_for_match(s: str) -> str:
    s = s.replace("\r"," ").strip()
    s = re.sub(r"\s+"," ",s)
    return s

def _anchor_to_regex(anchor: str) -> re.Pattern:
    a=_normalize_for_match(anchor)
    esc=re.escape(a); esc=re.sub(r"\\\s+", r"\\s+", esc)
    return re.compile(esc, re.IGNORECASE|re.UNICODE)

def _build_manifest_for_batch(batch_pages: List[Dict[str, Any]]) -> str:
    """T·∫°o manifest <<<PAGE i>>> cho 1 l√¥ (ƒë√°nh s·ªë trang 1..len(batch))."""
    lines=[]
    for i, p in enumerate(batch_pages, start=1):
        t=(p.get("text") or "").rstrip()
        lines.append(f"<<<PAGE {i}>>>")
        lines.append(t)
        lines.append("")
    return "\n".join(lines)


# ===================== GEMINI: ONE-SHOT PLAN (ANCHORS ‚Äì H·∫†T M·ªäN) =====================
def one_shot_plan_for_doc_pages_by_sent_anchors(doc_id: str, pages: List[Dict[str, Any]]) -> Dict[str, Any]:
    model = genai.GenerativeModel(SEGMENT_MODEL)

    all_segments = []
    total_pages = len(pages)
    if total_pages == 0:
        return {"segments": []}

    # L·∫∑p theo l√¥ 10 trang
    for batch_start in range(0, total_pages, PLAN_BATCH_PAGES):
        batch_end = min(total_pages, batch_start + PLAN_BATCH_PAGES)
        batch_pages = pages[batch_start:batch_end]
        local_count = len(batch_pages)

        manifest = _build_manifest_for_batch(batch_pages)

        system_prompt = (
            "B·∫°n l√† c√¥ng c·ª• L·∫¨P K·∫æ HO·∫†CH ph√¢n ƒëo·∫°n THEO TRANG cho vƒÉn b·∫£n ph√°p quy.\n"
            "ƒê·∫ßu v√†o l√† m·ªôt NH√ìM TRANG c√≥ ƒë√°nh d·∫•u '<<<PAGE k>>>', v·ªõi k ch·∫°y t·ª´ 1 ƒë·∫øn {N} (ch·ªâ trong l√¥ n√†y).\n"
            "H√£y chia th√†nh NHI·ªÄU ƒëo·∫°n (segment) m·∫°ch l·∫°c, M·ªñI segment ~{T} k√Ω t·ª± (t·ªëi thi·ªÉu {MIN}, t·ªëi ƒëa {MAX}); "
            "n·∫øu d√†i h∆°n {MAX} PH·∫¢I t√°ch nh·ªè. KH√îNG ƒë∆∞·ª£c g·ªôp c·∫£ CH∆Ø∆†NG n·∫øu v∆∞·ª£t {MAX}.\n"
            "M·ªói segment b·∫Øc qua t·ªëi ƒëa {SP} trang TRONG L√î n√†y; KH√îNG THAM CHI·∫æU sang trang ngo√†i l√¥.\n"
            "TR·∫¢ V·ªÄ JSON K·∫æ HO·∫†CH V·ªöI ANCHORS c√¢u ƒë·∫ßu/cu·ªëi cho t·ª´ng span:\n"
            "{{\"segments\":[{{\"title\":\"ng·∫Øn g·ªçn\",\"spans\":[{{\"page\":1,\"begin\":\"...\",\"end\":\"...\"}}, ...]}} , ...]}}\n"
            "- 'page' l√† CH·ªà S·ªê TRONG L√î (1..{N}); 'begin'/'end' l√† NGUY√äN VƒÇN c√¢u ƒë·∫ßu/cu·ªëi c·∫Øt ng·∫Øn ‚â§ {A} k√Ω t·ª±."
        ).format(N=local_count, T=TARGET_SEG_CHARS, MIN=MIN_SEG_CHARS, MAX=MAX_SEG_CHARS,
                 SP=MAX_SPANS_PER_SEG, A=ANCHOR_MAX_CHARS)

        user_prompt = "D∆∞·ªõi ƒë√¢y l√† n·ªôi dung theo TRANG c·ªßa l√¥ hi·ªán t·∫°i. H√£y tr·∫£ v·ªÅ k·∫ø ho·∫°ch ƒë√∫ng y√™u c·∫ßu:\n\n" + manifest

        # G·ªçi LLM
        resp = model.generate_content([system_prompt, user_prompt])
        txt = (resp.text or "").strip()
        if SAVE_PLAN_RAW:
            os.makedirs("segments", exist_ok=True)
            with open(os.path.join("segments", f"{doc_id}.plan.batch_{batch_start+1}_{batch_end}.raw.txt"),
                      "w", encoding="utf-8") as f:
                f.write(_sanitize_for_log(txt))

        plan_local = _safe_json_loads(txt)

        # Fallback n·∫øu JSON h·ªèng: 1 segment gom c·∫£ l√¥, neo ƒë·∫ßu/cu·ªëi m·ªói trang trong l√¥
        if not plan_local or "segments" not in plan_local:
            spans=[]
            for i, p in enumerate(batch_pages, start=1):
                t=(p.get("text") or "")
                t_norm=_normalize_for_match(t)
                if not t_norm: continue
                begin=t_norm[:ANCHOR_MAX_CHARS]; end=t_norm[-ANCHOR_MAX_CHARS:]
                spans.append({"page": i, "begin": begin, "end": end})
            plan_local={"segments":[{"title":"", "spans":spans}]}

        # ƒêi·ªÅu ch·ªânh 'page' t·ª´ LOCAL ‚Üí GLOBAL (c·ªông offset batch)
        page_offset = batch_start  # v√¨ local page b·∫Øt ƒë·∫ßu 1 ‚Üí global = offset + local
        for seg in plan_local.get("segments", []):
            fixed_spans=[]
            for sp in seg.get("spans", []):
                try:
                    local_pid = int(sp["page"])
                except Exception:
                    continue
                global_pid = local_pid + page_offset
                # B·∫£o v·ªá bi√™n
                if 1 <= global_pid <= total_pages:
                    sp2 = dict(sp)
                    sp2["page"] = global_pid
                    fixed_spans.append(sp2)
            if fixed_spans:
                all_segments.append({
                    "title": (seg.get("title") or "").strip(),
                    "spans": fixed_spans
                })

    # G·ªôp th√†nh 1 k·∫ø ho·∫°ch chung
    out_plan = {"segments": all_segments}

    if SAVE_PLAN:
        os.makedirs("segments", exist_ok=True)
        with open(os.path.join("segments", f"{doc_id}.plan.json"), "w", encoding="utf-8") as f:
            f.write(json.dumps(out_plan, ensure_ascii=False, indent=2))
        print(f"   ‚Ü≥ ƒê√£ l∆∞u k·∫ø ho·∫°ch (anchors, theo l√¥ {PLAN_BATCH_PAGES} trang): segments/{doc_id}.plan.json")

    return out_plan

# ===================== MATERIALIZE T·ª™ ANCHORS =====================
def apply_page_anchor_plan_to_segments(plan: Dict[str, Any], pages: List[Dict[str, Any]], joiner: str="\n\n", keep_prov: bool=True) -> List[Dict[str, Any]]:
    out=[]
    for seg in plan.get("segments", []):
        title=(seg.get("title") or "").strip()
        spans=sorted(seg.get("spans", []), key=lambda sp: int(sp["page"]))

        parts=[]; pf=None; pt=None; prov=[]
        for idx, sp in enumerate(spans):
            pid=int(sp["page"])
            if not (1<=pid<=len(pages)): continue
            raw=pages[pid-1].get("text") or ""
            hay_norm=_normalize_for_match(raw)

            begin=(sp.get("begin") or "")[:ANCHOR_MAX_CHARS]
            end  =(sp.get("end")   or "")[:ANCHOR_MAX_CHARS]

            # t√¨m begin (ƒë·∫ßu ti√™n) v√† end (cu·ªëi c√πng) tolerant kho·∫£ng tr·∫Øng
            s_idx=0; e_idx=len(raw)
            if begin:
                m=list(_anchor_to_regex(begin).finditer(hay_norm))
                if m: s_idx=m[0].start()
            if end:
                m=list(_anchor_to_regex(end).finditer(hay_norm))
                if m: e_idx=max(s_idx+1, m[-1].end())

            if SNAP_TO_BOUNDARY and s_idx<e_idx:
                s_idx, e_idx = _snap_span_to_word_sentence(raw, s_idx, min(e_idx, len(raw)))

            chunk=raw[s_idx:e_idx]

            if idx>0 and parts and parts[-1] and _needs_joiner(parts[-1]): parts.append(joiner)
            parts.append(chunk)

            pg=pages[pid-1]["page_num"]
            pf = pg if pf is None else min(pf, pg)
            pt = pg if pt is None else max(pt, pg)

            if keep_prov:
                prov.append({"page": pid, "begin": begin, "end": end, "start": s_idx, "finish": e_idx})

        text="".join(parts).strip()
        if text:
            rec={"title": title, "text": text, "page_from": pf or 0, "page_to": pt or 0}
            if keep_prov: rec["spans"]=prov
            out.append(rec)
    return out

# ===================== H·∫¨U KI·ªÇM & T·ª∞ CHIA NH·ªé =====================
_SENT_SPLIT = re.compile(r"(?<=[\.\!\?\:;])\s+(?=[A-Z√Ä-·ª≤ƒÄƒÇƒêƒ®≈®∆†∆Ø·∫†-·ªπ])", re.UNICODE)
def _split_into_paras(text: str) -> List[str]:
    parts=[p.strip() for p in text.split("\n\n")]
    return [p for p in parts if p]

def _split_into_sentences(text: str) -> List[str]:
    sents = _SENT_SPLIT.split(text.strip())
    return [s.strip() for s in sents if s.strip()]

def _natural_refine(text: str, target: int, min_len: int, max_len: int) -> List[str]:
    """Chia text th√†nh c√°c m·∫£nh g·∫ßn target; ∆∞u ti√™n ng·∫Øt ƒëo·∫°n, sau ƒë√≥ c√¢u; ƒë·∫£m b·∫£o min/max."""
    if len(text) <= max_len:
        return [text.strip()]

    paras=_split_into_paras(text)
    if len(paras)<=1:
        # kh√¥ng c√≥ ng·∫Øt ƒëo·∫°n ‚Üí chia theo c√¢u
        sents=_split_into_sentences(text)
        chunks=[]; buf=""
        for s in sents:
            if not buf: buf=s
            elif len(buf)+1+len(s) <= max_len:
                buf = buf + " " + s
            else:
                chunks.append(buf.strip()); buf=s
        if buf: chunks.append(buf.strip())
        # g·ªôp c√°c m·∫£nh ng·∫Øn ƒë·ªÉ ƒë·∫°t t·ªëi thi·ªÉu
        merged=[]; cur=""
        for c in chunks:
            if not cur: cur=c
            elif len(cur) < min_len or len(cur)+2+len(c) < min_len:
                cur = cur + "\n\n" + c
            else:
                merged.append(cur.strip()); cur=c
        if cur: merged.append(cur.strip())
        return merged

    # c√≥ ng·∫Øt ƒëo·∫°n ‚Üí g·ªôp ƒëo·∫°n g·∫ßn target
    chunks=[]; cur=""
    for p in paras:
        if not cur:
            cur=p
        elif len(cur)+2+len(p) <= max_len:
            cur = cur + "\n\n" + p
        else:
            # n·∫øu cur c√≤n qu√° nh·ªè, th·ª≠ gh√©p m·∫°nh tay th√™m 1 ƒëo·∫°n
            if len(cur) < min_len and len(cur)+2+len(p) <= (max_len + max_len//5):
                cur = cur + "\n\n" + p
            else:
                chunks.append(cur.strip()); cur=p
    if cur: chunks.append(cur.strip())

    # n·∫øu m·∫£nh n√†o v·∫´n > max_len, r·∫°ch ti·∫øp theo c√¢u
    refined=[]
    for c in chunks:
        if len(c) > max_len:
            refined.extend(_natural_refine(c, target, min_len, max_len))
        else:
            refined.append(c)
    return refined

def refine_segments(segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """B·∫£o ƒë·∫£m m·ªçi segment n·∫±m trong [MIN_SEG_CHARS, MAX_SEG_CHARS] (tr·ª´ segment ch·ªâ ti√™u ƒë·ªÅ)."""
    out=[]
    for seg in segments:
        txt=seg["text"]; title=seg.get("title","").strip()
        if len(txt) <= MAX_SEG_CHARS:
            out.append(seg); continue

        # t√°ch t·ª± nhi√™n
        parts=_natural_refine(txt, TARGET_SEG_CHARS, MIN_SEG_CHARS, MAX_SEG_CHARS)
        for i, piece in enumerate(parts, start=1):
            rec=dict(seg)
            rec["text"]=piece
            rec["title"]= f"{title} (ph·∫ßn {i})" if title else f"(ph·∫ßn {i})"
            # provenance: gi·ªØ page_from/to c·ªßa b·∫£n g·ªëc
            out.append(rec)
    return out

# ===================== SEGMENT DRIVER (ANCHORS + REFINE) =====================
def segment_docs_pages_with_anchors(pages_by_doc: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
    out_all={}
    os.makedirs("segments", exist_ok=True)
    for doc_id, pages in pages_by_doc.items():
        print(f"üìë {doc_id}: {len(pages)} trang ‚Üí anchors (h·∫°t m·ªãn)")
        plan_path=os.path.join("segments", f"{doc_id}.plan.json")
        if USE_EXISTING_PLAN and os.path.isfile(plan_path):
            with open(plan_path,"r",encoding="utf-8") as f: plan=json.load(f)
            print(f"   ‚Ü≥ D√πng k·∫ø ho·∫°ch c√≥ s·∫µn: {plan_path}")
        else:
            plan=one_shot_plan_for_doc_pages_by_sent_anchors(doc_id, pages)

        segs = apply_page_anchor_plan_to_segments(plan, pages, joiner="\n\n", keep_prov=True)
        segs_refined = refine_segments(segs)
        print(f"   ‚Üí {len(segs)} segments (raw) ‚Üí {len(segs_refined)} segments (refined)")
        out_all[doc_id]=segs_refined

        if SAVE_SEGMENT and segs_refined:
            with open(os.path.join("segments", f"{doc_id}.jsonl"), "w", encoding="utf-8") as f:
                for s in segs_refined: f.write(json.dumps(s, ensure_ascii=False) + "\n")
            print(f"   ‚Ü≥ ƒê√£ l∆∞u segments/{doc_id}.jsonl")
    return out_all

# ===================== ELASTICSEARCH INDEXING =====================
def ensure_index(es: Elasticsearch, index_name: str, dims: int) -> None:
    try:
        if es.indices.exists(index=index_name): return
    except Exception: pass
    body={
        "settings":{"number_of_shards":1,"number_of_replicas":0},
        "mappings":{
            "properties":{
                "doc_id":{"type":"keyword"},
                "title":{"type":"text"},
                "text":{"type":"text"},
                "page_from":{"type":"integer"},
                "page_to":{"type":"integer"},
                "ingested_at":{"type":"date"},
                "vector":{"type":"dense_vector","dims":dims,"index":True,"similarity":"cosine"}
            }
        }
    }
    es.indices.create(index=index_name, body=body)
    print(f"üÜï ƒê√£ t·∫°o index '{index_name}'")

def embed_text(text: str, max_retries: int=4) -> List[float]:
    for attempt in range(max_retries):
        try:
            r=genai.embed_content(model=EMBED_MODEL, content=text, task_type="retrieval_document")
            vec=r.get("embedding", [])
            if not vec: raise RuntimeError("Empty embedding")
            return vec
        except Exception:
            if attempt==max_retries-1: raise
            time.sleep((2**attempt)+random.uniform(0,0.4))
    return []

def iter_actions_for_bulk(segments_by_doc: Dict[str, List[Dict[str, Any]]]):
    for doc_id, segs in segments_by_doc.items():
        for i, s in enumerate(segs):
            title=(s.get("title") or "").strip(); text=(s.get("text") or "").strip()
            if not text: continue
            vec=embed_text(text)
            _id=f"{doc_id}-{s.get('page_from','')}-{s.get('page_to','')}-{i}"
            yield {
                "_op_type":"index","_id":_id,"_index":ES_INDEX,
                "doc_id":doc_id,"title":title,"text":text,
                "page_from":int(s.get("page_from") or 0),
                "page_to":int(s.get("page_to") or 0),
                "ingested_at":datetime.utcnow().isoformat(timespec="seconds")+"Z",
                "vector":vec
            }

def embed_and_index(segments_by_doc: Dict[str, List[Dict[str, Any]]]) -> Tuple[int,int]:
    configure_gemini(); es=get_es()
    probe=genai.embed_content(model=EMBED_MODEL, content="probe")
    dims=len(probe.get("embedding", [])) or 3072
    ensure_index(es, ES_INDEX, dims)

    print("üöÄ B·∫Øt ƒë·∫ßu embed + index ‚Ä¶")
    success, fail = 0, 0
    batch=[]; BATCH_SIZE=200
    for act in iter_actions_for_bulk(segments_by_doc):
        batch.append(act)
        if len(batch)>=BATCH_SIZE:
            ok, err = helpers.bulk(es, batch, raise_on_error=False)
            success += ok; fail += len(err); batch.clear()
    if batch:
        ok, err = helpers.bulk(es, batch, raise_on_error=False)
        success += ok; fail += len(err)
    print(f"‚úÖ Indexed: {success} | ‚ùå Errors: {fail}")
    return success, fail

# ===================== MAIN =====================
if __name__ == "__main__":
    print("=== HEALTH CHECK ==="); check_connections()
    print("\n=== READ & CLEAN PDFs ==="); pages_by_doc = read_and_clean_pdfs()
    print("\n=== SEGMENT (ANCHORS + REFINE) ==="); segments_by_doc = segment_docs_pages_with_anchors(pages_by_doc)
    print("\n=== EMBED + INDEX ==="); embed_and_index(segments_by_doc)
