"""
Main orchestrator: nh·∫≠n c√¢u h·ªèi ng∆∞·ªùi d√πng ‚Üí g·ªçi b·ªô l·∫≠p k·∫ø ho·∫°ch (file #1)
‚Üí th·ª±c thi theo k·∫ø ho·∫°ch: SQL_ONLY | RAG_ONLY | HYBRID.

Y√™u c·∫ßu m√¥i tr∆∞·ªùng:
- Python packages: google-generativeai, elasticsearch, pyodbc (ho·∫∑c pymssql), python-dotenv
- ENV cho Gemini: GEMINI_API_KEY
- ENV cho SQL Server (n·∫øu ch·∫°y SQL):
  SQLSERVER_HOST, SQLSERVER_PORT (m·∫∑c ƒë·ªãnh 1433), SQLSERVER_DB,
  SQLSERVER_USER, SQLSERVER_PASSWORD, SQLSERVER_DRIVER (v√≠ d·ª•: "ODBC Driver 17 for SQL Server")

Gi·∫£ ƒë·ªãnh:
- File #1 (planner) c√≥ c√°c h√†m: configure_gemini(), load_context_from_folders(list[str]) -> str,
  get_execution_plan(user_query: str, full_context: str) -> dict | None
- File #2 (retrieval) c√≥ h√†m: run_qa_pipeline(query_text: str, keywords: str | None = None, filters: dict | None = None,
  initial_top_k: int = 5, max_top_k_cap: int = 50, max_iters: int = 4)

C·∫≠p nh·∫≠t import cho ƒë√∫ng t√™n file c·ªßa b·∫°n.
"""
from __future__ import annotations
import os
import re
import json
from typing import Any, Dict, List, Optional, Tuple
from dotenv import load_dotenv

# ==== C·∫¨P NH·∫¨T L·∫†I CHO ƒê√öNG T√äN MODULE C·ª¶A B·∫†N ====
# V√≠ d·ª•, n·∫øu file #1 t√™n "planner.py" v√† file #2 t√™n "retrieval.py":
# from planner import configure_gemini, load_context_from_folders, get_execution_plan
# from retrieval import run_qa_pipeline
#
# N·∫øu t√™n kh√°c, s·ª≠a l·∫°i 2 d√≤ng import d∆∞·ªõi cho kh·ªõp:
from Orchestrator.task import orchestrator, fix_sql_error_with_gemini  # type: ignore
from scripts.rag_pipeline import run_qa_pipeline  # type: ignore  # n·∫øu b·∫°n ƒë√£ ƒë·ªÉ run_qa_pipeline trong file test.py


# ---------- STUB SQL EXECUTOR (b·∫°n s·∫Ω b·ªï sung sau) ----------
from typing import List, Tuple, Any
import pyodbc
import google.generativeai as genai
def configure_gemini():
    """C·∫•u h√¨nh API key cho Gemini t·ª´ bi·∫øn m√¥i tr∆∞·ªùng."""
    load_dotenv()
    try:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("Bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p.")
        genai.configure(api_key=api_key)
    except Exception as e:
        print(f"L·ªói khi c·∫•u h√¨nh Gemini: {e}")
        exit()
        
def _split_batches(sql: str) -> List[str]:
    parts, buf = [], []
    for line in sql.splitlines():
        if re.match(r'^\s*GO\s*$', line, flags=re.IGNORECASE):
            if buf:
                parts.append("\n".join(buf).strip())
                buf = []
        else:
            buf.append(line)
    if buf:
        parts.append("\n".join(buf).strip())
    return [p for p in parts if p]

def execute_sql_query(sql: str) -> Tuple[List[str], List[Tuple[Any, ...]]]:
    """
    Th·ª±c thi T-SQL b·∫±ng Windows Authentication t·ªõi CNTT @ SQLEXPRESS.
    - H·ªó tr·ª£ script nhi·ªÅu batch c√≥ 'GO'
    - Tr·∫£ v·ªÅ (columns, rows) c·ªßa result set CU·ªêI C√ôNG
    - N·∫øu l·ªói: raise Exception v·ªõi th√¥ng b√°o l·ªói chi ti·∫øt
    """
    conn = None
    try:
        conn = pyodbc.connect(
            r"DRIVER={ODBC Driver 18 for SQL Server};"
            r"SERVER=localhost\SQLEXPRESS;"
            r"DATABASE=CNTT;"
            r"Trusted_Connection=yes;"
            r"Encrypt=yes;TrustServerCertificate=yes;",
            autocommit=True,
            timeout=15,
        )
        cur = conn.cursor()

        batches = _split_batches(sql) or [sql]
        last_cols: List[str] = []
        last_rows: List[Tuple[Any, ...]] = []

        for b in batches:
            try:
                cur.execute(b)
            except pyodbc.Error as e:
                # Tr·∫£ l·ªói c·ª• th·ªÉ cho t·ª´ng batch
                raise Exception(f"L·ªói SQL ·ªü batch:\n{b}\nChi ti·∫øt: {e}")

            # X·ª≠ l√Ω result set ch√≠nh
            if cur.description is not None:
                cols = [c[0] for c in cur.description]
                data = cur.fetchall()
                last_cols, last_rows = cols, [tuple(r) for r in data]
            else:
                last_cols, last_rows = ["rows_affected"], [(cur.rowcount,)]

            # X·ª≠ l√Ω c√°c result set ti·∫øp theo (n·∫øu c√≥)
            while cur.nextset():
                if cur.description is not None:
                    cols = [c[0] for c in cur.description]
                    data = cur.fetchall()
                    last_cols, last_rows = cols, [tuple(r) for r in data]
                else:
                    last_cols, last_rows = ["rows_affected"], [(cur.rowcount,)]

        return last_cols, last_rows

    except pyodbc.Error as e:
        # Tr·∫£ l·ªói ra ngo√†i ƒë·ªÉ Gemini x·ª≠ l√Ω
        raise Exception(f"L·ªói khi th·ª±c thi SQL:\n{e}")

    finally:
        try:
            if conn:
                conn.close()
        except:
            pass

        
def _print_sql_result(cols: List[str], rows: List[Tuple[Any, ...]], limit: int = 50) -> None:
    if not cols:
        print("‚õî Kh√¥ng c√≥ c·ªôt/k·∫øt qu·∫£.")
        return
    print("\n--- K·∫æT QU·∫¢ SQL ---")
    print(", ".join(cols))
    for i, r in enumerate(rows[:limit], 1):
        print(f"{i:>3}: " + ", ".join([str(v) for v in r]))
    if len(rows) > limit:
        print(f"... ({len(rows) - limit} d√≤ng n·ªØa)")

# ---------- Helper ----------


def print_plan(plan: Dict[str, Any]) -> None:
    print("\n--- K·∫æ HO·∫†CH ---")
    print(json.dumps(plan, indent=2, ensure_ascii=False))


# ---- Chu·∫©n h√≥a rows SQL -> JSON cho LLM d·ªÖ hi·ªÉu ----
def _rows_to_json(columns: list[str], rows: list[tuple]) -> list[dict]:
    if not columns or not rows:
        return []
    out = []
    for r in rows:
        out.append({columns[i]: (r[i] if i < len(r) else None) for i in range(len(columns))})
    return out

# ---- G·ªçi Gemini ƒë·ªÉ t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi cu·ªëi ----
FINAL_MODEL = os.getenv("MODEL_FINAL", "models/gemini-2.5-pro")

def _finalize_answer_llm(user_query: str,
                         sql_columns: list[str] | None,
                         sql_rows: list[tuple] | None,
                         rag_text: str | None) -> str:
    sql_json = _rows_to_json(sql_columns or [], sql_rows or [])
    prompt = (
        "B·∫°n l√† tr·ª£ l√Ω h·ªçc v·ª•. H√£y t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi NG·∫ÆN G·ªåN, R√ï R√ÄNG t·ª´ hai ngu·ªìn sau.\n"
        "- SQL_DATA: d·ªØ li·ªáu b·∫£ng (n·∫øu c√≥) - tr√≠ch ra c√°c ƒëi·ªÉm/ch·ªâ s·ªë quan tr·ªçng.\n"
        "- RAG_SNIPPET: tr√≠ch l∆∞·ª£c quy ƒë·ªãnh/ch√≠nh s√°ch - d√πng l√†m cƒÉn c·ª© gi·∫£i th√≠ch.\n"
        "- N·∫øu thi·∫øu d·ªØ li·ªáu ƒë·ªÉ k·∫øt lu·∫≠n, n√≥i r√µ ph·∫ßn n√†o c√≤n thi·∫øu.\n\n"
        f"USER_QUERY:\n{user_query}\n\n"
        f"SQL_DATA_JSON:\n{json.dumps(sql_json, ensure_ascii=False)}\n\n"
        f"RAG_SNIPPET:\n{rag_text or ''}\n\n"
        "Y√äU C·∫¶U ƒê·∫¶U RA: Tr·∫£ l·ªùi ti·∫øng Vi·ªát, t·ªëi ƒëa ~8 g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c 1 ƒëo·∫°n ng·∫Øn; "
        "n·∫øu c·∫ßn li·ªát k√™ ngu·ªìn, th√™m d√≤ng cu·ªëi 'Ngu·ªìn: ...'."
    )
    try:
        import google.generativeai as genai
        # configure_gemini() ƒë√£ ƒë∆∞·ª£c g·ªçi ·ªü tr√™n, n√™n genai ƒë√£ c√≥ API key
        model = genai.GenerativeModel(FINAL_MODEL)
        resp = model.generate_content(prompt, generation_config={"temperature": 0.2})
        return (resp.text or "").strip()
    except Exception as e:
        return f"[Finalizer error] {e}"
    
def _finalize_answer_with_gemini(
    user_query: str,
    sql_columns: Optional[List[str]] = None,
    sql_rows: Optional[List[Tuple[Any, ...]]] = None,
    rag_text: Optional[str] = None,
) -> Dict[str, Any]:
    """
    T·ªïng h·ª£p k·∫øt qu·∫£ t·ª´ SQL v√† RAG ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô ƒë·ªß th√¥ng tin.
    - N·∫øu ch·ªâ c√≥ SQL ‚Üí sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu SQL.
    - N·∫øu ch·ªâ c√≥ RAG ‚Üí ki·ªÉm tra ƒë·ªß th√¥ng tin, n·∫øu thi·∫øu ‚Üí INSUFFICIENT_INFO.
    - N·∫øu c√≥ c·∫£ hai ‚Üí sinh c√¢u tr·∫£ l·ªùi k·∫øt h·ª£p gi·ªØa SQL v√† RAG.
    Tr·∫£ v·ªÅ:
        {
            "status": "ANSWER" | "INSUFFICIENT_INFO" | "ERROR",
            "answer": str,
            "reason": str,
        }
    """
    try:
        # 1Ô∏è‚É£ Chuy·ªÉn SQL k·∫øt qu·∫£ th√†nh JSON d·ªÖ ƒë·ªçc
        sql_json = []
        if sql_columns and sql_rows:
            sql_json = [
                {sql_columns[i]: (r[i] if i < len(r) else None) for i in range(len(sql_columns))}
                for r in sql_rows
            ]

        # 2Ô∏è‚É£ Ki·ªÉm tra n·∫øu c·∫£ 2 ƒë·ªÅu r·ªóng
        if not sql_json and not rag_text:
            return {
                "status": "INSUFFICIENT_INFO",
                "reason": "Kh√¥ng c√≥ d·ªØ li·ªáu t·ª´ SQL ho·∫∑c RAG ƒë·ªÉ ƒë√°nh gi√°.",
                "answer": "",
            }

        # 3Ô∏è‚É£ X√¢y d·ª±ng context ƒë·∫ßu v√†o
        context_blocks = []
        if sql_json:
            context_blocks.append(f"üìò SQL_RESULT_JSON:\n{json.dumps(sql_json, ensure_ascii=False, indent=2)}")
        if rag_text:
            context_blocks.append(f"üìó RAG_SNIPPET:\n{rag_text.strip()}")

        full_context = "\n\n".join(context_blocks)

        # 4Ô∏è‚É£ Prompt cho Gemini
        prompt = f"""
B·∫°n l√† tr·ª£ l√Ω h·ªçc v·ª• t·∫°i tr∆∞·ªùng ƒë·∫°i h·ªçc. D∆∞·ªõi ƒë√¢y l√† c√¢u h·ªèi v√† d·ªØ li·ªáu ƒë∆∞·ª£c truy xu·∫•t t·ª´ hai ngu·ªìn:

C√ÇU H·ªéI:
{user_query}

--- D·ªÆ LI·ªÜU TRUY XU·∫§T ---
{full_context}
---------------------------

Nhi·ªám v·ª•:
1. Ph√¢n t√≠ch xem d·ªØ li·ªáu hi·ªán c√≥ (SQL, RAG ho·∫∑c c·∫£ hai) c√≥ ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n hay kh√¥ng.
2. N·∫øu ch·ªâ c√≥ SQL ‚Üí h√£y vi·∫øt chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu d·∫°ng b·∫£ng th√†nh ƒëo·∫°n vƒÉn.
3. N·∫øu ch·ªâ c√≥ RAG ‚Üí h√£y ki·ªÉm tra xem RAG ƒë√£ ƒë·ªß ch∆∞a. N·∫øu ch∆∞a ƒë·ªß ‚Üí tr·∫£ v·ªÅ tr·∫°ng th√°i "INSUFFICIENT_INFO".
4. N·∫øu c√≥ c·∫£ SQL v√† RAG ‚Üí sinh c√¢u tr·∫£ l·ªùi k·∫øt h·ª£p, s·ª≠ d·ª•ng th√¥ng tin t·ª´ RAG v√† ƒë·ªëi chi·∫øu / b·ªï sung b·∫±ng d·ªØ li·ªáu SQL n·∫øu c·∫ßn.

Y√äU C·∫¶U ƒê·∫¶U RA (JSON CHU·∫®N):
```json
{{
  "status": "ANSWER" | "INSUFFICIENT_INFO",
  "answer": "C√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn n·∫øu ƒë·ªß d·ªØ li·ªáu, n·∫øu ch∆∞a ƒë·ªß th√¨ ƒë·ªÉ tr·ªëng",
  "reason": "Gi·∫£i th√≠ch v√¨ sao ƒë·ªß ho·∫∑c ch∆∞a ƒë·ªß"
}}
"""
        model = genai.GenerativeModel("gemini-2.5-pro")
        response = model.generate_content(prompt, generation_config={"temperature": 0.2})
        text = (response.text or "").strip()

        if text.startswith("```json"): text = text[7:]
        if text.endswith("```"): text = text[:-3]
        parsed = json.loads(text)

        status = parsed.get("status", "").upper()
        if status not in ["ANSWER", "INSUFFICIENT_INFO"]:
            return {"status": "ERROR", "reason": f"Tr·∫°ng th√°i kh√¥ng h·ª£p l·ªá: {status}", "answer": ""}

        parsed["answer"] = (parsed.get("answer") or "").strip()
        parsed["reason"] = (parsed.get("reason") or "").strip()
        return parsed

    except Exception as e:
        return {"status": "ERROR", "reason": f"L·ªói trong qu√° tr√¨nh t·ªïng h·ª£p: {e}", "answer": ""}


# ---------- Orchestrator ----------
def run_orchestrator(
    user_query: str,
) -> None:
    """
    1) N·∫°p ng·ªØ c·∫£nh (cho planner)
    2) G·ªçi get_execution_plan(user_query, full_context)
    3) Th·ª±c thi theo query_type: SQL_ONLY | RAG_ONLY | HYBRID
       - SQL: g·ªçi execute_sql_query(sql) [stub]
       - RAG: g·ªçi run_qa_pipeline(query_text, keywords, filters)
    """
    load_dotenv()


    plan = orchestrator(user_query)

    sql_query: Optional[str] = plan.get("sql_query")
    kws: Optional[List[str]] = plan.get("keywords_for_rag")
    max_attempt = 3  # s·ªë l·∫ßn g·ªçi l·∫°i t·ªëi ƒëa n·∫øu kh√¥ng ƒë·ªß d·ªØ li·ªáu
    attempt = 1
    sql_cols, sql_rows = [], []
    # 3) Th·ª±c thi theo k·∫ø ho·∫°ch
    while attempt <= max_attempt:
        if sql_query:
            current_sql = sql_query
            check_error_sql = True

            while check_error_sql:
                try:
                    # Th·ª±c thi truy v·∫•n
                    sql_cols, sql_rows = execute_sql_query(current_sql)
                    _print_sql_result(sql_cols, sql_rows)
                    break  # d·ª´ng n·∫øu th√†nh c√¥ng

                except Exception as e:
                    # ‚ùå N·∫øu l·ªói, l·∫•y th√¥ng b√°o l·ªói chi ti·∫øt
                    sql_error = str(e)
                    # üîß G·ªçi Gemini ƒë·ªÉ s·ª≠a truy v·∫•n
                    try:
                        print("\nüîß G·ªçi Gemini ƒë·ªÉ s·ª≠a truy v·∫•n...")
                        fixed_sql = fix_sql_error_with_gemini(
                            question=user_query,
                            sql_query=current_sql,
                            sql_error=sql_error,
                        )
                        print(f"\nGemini ƒë·ªÅ xu·∫•t truy v·∫•n m·ªõi:\n{fixed_sql}")
                        current_sql = fixed_sql  # c·∫≠p nh·∫≠t c√¢u m·ªõi ƒë·ªÉ th·ª≠ l·∫°i
                    except Exception as e2:
                        print(f"L·ªói khi g·ªçi Gemini s·ª≠a SQL: {e2}")
                        break
            else:
                print("H·∫øt s·ªë l·∫ßn th·ª≠, kh√¥ng th·ªÉ s·ª≠a truy v·∫•n.")
            
        if kws: #kws != null
            print("\nüîç Ch·∫°y RAG v·ªõi t·ª´ kh√≥a:", kws)
            result = run_qa_pipeline(
                query_text=user_query,
                keywords=kws,
                filters=None,
                initial_top_k=5,
                max_top_k_cap=50,
                max_iters=4,
            )
            
        #Todo: Finalize answer
        final_eval = _finalize_answer_with_gemini(
            user_query=user_query,
            sql_columns=sql_cols,
            sql_rows=sql_rows,
            rag_text=result.get("answer", "") if kws else None,
        )

        status = final_eval.get("status")
        if status == "ANSWER":
            print("\n‚úÖ C√ÇU TR·∫¢ L·ªúI CU·ªêI C√ôNG ===")
            print(final_eval["answer"])
            return
        elif status == "INSUFFICIENT_INFO":
            print(f"\nD·ªØ li·ªáu ch∆∞a ƒë·ªß: {final_eval['reason']}")
            # G·ªçi orchestrator(user_query) ho·∫∑c sinh keyword m·ªõi
            attempt += 1
        else:
            print(f"\nL·ªói khi t·ªïng h·ª£p: {final_eval.get('reason')}")
            return
    
    

# ---------- Entry ----------
if __name__ == "__main__":
    try:
        # q = input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n: ").strip()
        # q = "ƒëi·ªÅu ki·ªán x√©t tuy·ªÉn th·∫≥ng"
        q = "Cho em h·ªèi ƒëi·ªÅu ki·ªán ƒë·ªÉ h·ªçc m√¥n C·∫•u tr√∫c d·ªØ li·ªáu v√† gi·∫£i thu·∫≠t l√† g√¨?"
    except EOFError:
        q = ""
    if not q:
        q = "Cho em h·ªèi ƒëi·ªÅu ki·ªán ƒë·ªÉ h·ªçc m√¥n C·∫•u tr√∫c d·ªØ li·ªáu v√† gi·∫£i thu·∫≠t l√† g√¨?"
        print(f"(D√πng m·∫∑c ƒë·ªãnh) {q}")

    # V√≠ d·ª• filter cho RAG (ƒë·ªÉ None n·∫øu mu·ªën t√¨m to√†n b·ªô)
    rag_filters_example = None
    # rag_filters_example = {"term": {"doc_id": "Quy_che_tuyen_sinh"}}

    run_orchestrator(
        user_query=q
    )