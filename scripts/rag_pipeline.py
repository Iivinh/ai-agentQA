import os, google.generativeai as genai
from typing import List, Dict, Any, Optional
from elasticsearch import Elasticsearch

GEMINI_API_KEY = "AIzaSyAJlOwMZm7n08S-vqfzgISw1P-0D-UcnlI"
EMBED_MODEL    = "models/gemini-embedding-001"
EMBED_DIMS     = 3072
genai.configure(api_key=GEMINI_API_KEY)

ES_URL = os.getenv("ES_URL", "http://localhost:9200")
ES_INDEX = "school_knowledge"
es = Elasticsearch(ES_URL)
TOP_K = 10
RRF_K = 60  # tham s·ªë RRF (rank_constant)
RRF_WINDOW = 50  # s·ªë doc l·∫•y t·ª´ m·ªói ngu·ªìn (BM25, kNN) ƒë·ªÉ fuse
KNN_NUM_CANDIDATES = 200  # s·ªë candidates ƒë·ªÉ kNN l·ªçc tr∆∞·ªõc khi l·∫•y top k
# ====== Embedding ======
def embed_query(text: str) -> List[float]:
    r = genai.embed_content(model=EMBED_MODEL, content=text)
    return r["embedding"]

# ... c√°c import & config nh∆∞ b·∫°n c√≥ ...
import os

# ·∫®n to√†n b·ªô log gRPC/absl
os.environ["GRPC_VERBOSITY"] = "NONE"
os.environ["GRPC_ABORT_ON_LEAKS"] = "false"

import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)

from typing import Iterable, Literal, Union



def hybrid_search(
    query_text: str,
    keywords: str | None = None,
    filters: dict | None = None,
    top_k: int = 10,
    rrf_window: int = 50,
    knn_num_candidates: int = 200,
    rrf_k: int = 60,
):
    # 1) BM25
    must = []
    if keywords:
        must.append({
            "multi_match": {
                "query": keywords,
                "fields": ["title^2", "text"],
                "operator": "and"
            }
        })

    bm25_body = {
        "size": rrf_window,
        "query": {
            "bool": {
                "must": must or [{"match_all": {}}],
                **({"filter": filters} if filters else {})
            }
        },
        "_source": ["title","page_from","page_to","text"]
    }
    bm25_res = es.search(index=ES_INDEX, body=bm25_body)
    bm25_hits = bm25_res.get("hits", {}).get("hits", [])

    # 2) kNN
    qvec = embed_query(query_text)
    knn_body = {
        "knn": {
            "field": "vector",
            "query_vector": qvec,
            "k": rrf_window,
            "num_candidates": knn_num_candidates
        },
        "_source": ["title","page_from","page_to","text"],
        "highlight": {"fields": {"text": {"fragment_size": 150, "number_of_fragments": 1}}}
    }
    if filters:
        knn_body["knn"]["filter"] = filters

    # ‚ö†Ô∏è perform_request tr·∫£ v·ªÅ TransportApiResponse -> c·∫ßn .body
    knn_raw = es.transport.perform_request(
        "POST",
        f"/{ES_INDEX}/_knn_search",
        headers={"Content-Type": "application/json"},
        body=knn_body,
    )
    knn_res = knn_raw.body if hasattr(knn_raw, "body") else knn_raw
    knn_hits = knn_res.get("hits", {}).get("hits", [])

    # 3) RRF fuse
    def rrf_fuse(bm25_hits, knn_hits, top_k=top_k, k=rrf_k):
        scores = {}
        # c·ªông 1/(k + rank)
        for rank, h in enumerate(bm25_hits, start=1):
            _id = h["_id"]; scores[_id] = scores.get(_id, 0.0) + 1.0/(k+rank)
        for rank, h in enumerate(knn_hits, start=1):
            _id = h["_id"]; scores[_id] = scores.get(_id, 0.0) + 1.0/(k+rank)

        # l·∫•y _source t·ª´ m·ªôt trong hai list
        idx = {h["_id"]: h for h in bm25_hits}
        idx.update({h["_id"]: h for h in knn_hits})

        out = []
        for _id, sc in sorted(scores.items(), key=lambda x: x[1], reverse=True)[:top_k]:
            src = idx[_id].get("_source", {})
            out.append({
                "_id": _id,
                "rrf_score": sc,
                "title": src.get("title"),
                "page_from": src.get("page_from"),
                "page_to": src.get("page_to"),
                "snippet": src.get("text")
            })
        return out
    
    return rrf_fuse(bm25_hits, knn_hits)

from collections import Counter
from typing import List, Dict, Any, Iterable, Optional

def _flatten_hits(hits_per_kw: Iterable[Any], pick_top1_per_kw: bool = True) -> List[Dict[str, Any]]:
    """
    hits_per_kw c√≥ th·ªÉ l√†:
      - List[Dict] (m·ªói ph·∫ßn t·ª≠ ƒë√£ l√† 1 hit)
      - List[List[Dict]] (m·ªói keyword tr·∫£ 1 list hit)
    pick_top1_per_kw=True: n·∫øu l√† list c√°c list th√¨ ch·ªâ l·∫•y hit ƒë·∫ßu m·ªói keyword
    """
    flat: List[Dict[str, Any]] = []
    for item in hits_per_kw:
        if isinstance(item, list):
            if not item:
                continue
            flat.append(item[0] if pick_top1_per_kw else item)  # n·∫øu False s·∫Ω th√™m c·∫£ list (√≠t d√πng)
        elif isinstance(item, dict):
            flat.append(item)
        else:
            # b·ªè qua ki·ªÉu l·∫°
            continue
    # n·∫øu c√≥ ph·∫ßn t·ª≠ l√† list (do pick_top1_per_kw=False), flatten n·ªët
    out: List[Dict[str, Any]] = []
    for x in flat:
        if isinstance(x, list):
            out.extend([h for h in x if isinstance(h, dict)])
        elif isinstance(x, dict):
            out.append(x)
    return out

def merge_hits_to_single_hit(
    hits_per_kw: Iterable[Any],
    max_snippet_chars: int = 1200,
    joiner: str = " ‚Ä¶ ",
    score_field_candidates: List[str] = ("rrf_score", "_score"),
) -> Optional[Dict[str, Any]]:
    """
    G·ªôp t·∫•t c·∫£ hit l·∫°i th√†nh 1 hit duy nh·∫•t:
      - id: n·ªëi c√°c _id b·∫±ng '+'
      - title: gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t (most common) trong c√°c hit c√≥ title
      - page_from/page_to: min / max trong c√°c hit c√≥ s·ªë trang
      - snippet: n·ªëi c√°c ƒëo·∫°n text/snippet (lo·∫°i tr√πng), c·∫Øt ƒë·ªô d√†i t·ªïng max_snippet_chars
      - kw: danh s√°ch t·ª´ kh√≥a (field 'kw' ho·∫∑c '_kw') unique
      - score: t·ªïng c√°c ƒëi·ªÉm (∆∞u ti√™n 'rrf_score', fallback '_score')
    """
    hits = _flatten_hits(hits_per_kw, pick_top1_per_kw=True)
    if not hits:
        return None

    # 1) gom tr∆∞·ªùng c∆° b·∫£n
    ids = [str(h.get("_id") or "") for h in hits if h.get("_id")]
    titles = [h.get("title") for h in hits if h.get("title")]
    pages_from = [h.get("page_from") for h in hits if isinstance(h.get("page_from"), (int, float))]
    pages_to   = [h.get("page_to")   for h in hits if isinstance(h.get("page_to"), (int, float))]
    kwords = []
    for h in hits:
        for key in ("kw", "_kw"):
            if h.get(key):
                kwords.append(str(h[key]))

    # 2) ch·ªçn title ph·ªï bi·∫øn nh·∫•t
    title_final = None
    if titles:
        cnt = Counter(titles)
        title_final = cnt.most_common(1)[0][0]

    # 3) snippet: ∆∞u ti√™n 'snippet', fallback '_source.text'; lo·∫°i tr√πng, n·ªëi l·∫°i
    seen = set()
    parts = []
    for h in hits:
        txt = h.get("snippet")
        if not txt:
            src = h.get("_source") or {}
            txt = src.get("text")
        if not txt:
            continue
        t = str(txt).strip()
        if t and t not in seen:
            seen.add(t)
            parts.append(t)
    merged_snippet = joiner.join(parts)
    if len(merged_snippet) > max_snippet_chars:
        merged_snippet = merged_snippet[:max_snippet_chars].rstrip() + " ‚Ä¶"

    # 4) ƒëi·ªÉm: t·ªïng c√°c score t·ª´ field ∆∞u ti√™n
    total_score = 0.0
    for h in hits:
        sc = None
        for f in score_field_candidates:
            if h.get(f) is not None:
                try:
                    sc = float(h.get(f))
                    break
                except Exception:
                    pass
        if sc is not None:
            total_score += sc

    merged = {
        "_id": "+".join(ids) if ids else None,
        "title": title_final,
        "page_from": min(pages_from) if pages_from else None,
        "page_to": max(pages_to) if pages_to else None,
        "snippet": merged_snippet if merged_snippet else None,
        "kw": sorted(set(kwords)) if kwords else None,
        "score_sum": total_score,
        "sources_count": len(hits),
    }
    return merged


# ====== LLM Orchestrator (Step 6) ======
import json
from typing import List, Dict, Any

# Cho ph√©p override model b·∫±ng ENV; m·∫∑c ƒë·ªãnh nh·∫Øm "2.5 pro" (ƒë·ªïi fallback n·∫øu c·∫ßn)
MODEL_QA = os.getenv("MODEL_QA", "models/gemini-2.5-flash")
def _format_context_for_prompt(hits: List[Any]) -> str:
    """
    Bi·∫øn danh s√°ch hits (c√≥ th·ªÉ l√† List[Dict] ho·∫∑c List[List[Dict]]) -> block context c√≥ ƒë√°nh s·ªë [cit:N].
    KH√îNG GI·ªöI H·∫†N ƒë·ªô d√†i context ‚Äî xu·∫•t to√†n b·ªô.
    """
    # üîß Flatten an to√†n: n·∫øu ph·∫ßn t·ª≠ l√† list ‚Üí m·ªü r·ªông, n·∫øu l√† dict ‚Üí gi·ªØ nguy√™n
    flat_hits = []
    for item in hits:
        if isinstance(item, list):
            flat_hits.extend(item)
        elif isinstance(item, dict):
            flat_hits.append(item)
        else:
            continue

    lines = []
    for i, h in enumerate(flat_hits, 1):
        if not isinstance(h, dict):
            continue

        title = str(h.get("title") or "").strip()
        pfrom = h.get("page_from") or "?"
        pto   = h.get("page_to") or "?"
        snippet = str(h.get("snippet") or "").strip().replace("\n", " ")

        # N·∫øu tr·ªëng c·∫£ title & snippet ‚Üí b·ªè qua
        if not (title or snippet):
            continue

        line = f"[#cit={i}] {title} ‚Äì p.{pfrom}‚Äì{pto}: {snippet}"
        lines.append(line)

    return "\n".join(lines)


def _build_qa_instruction() -> str:
    """
    R√†ng bu·ªôc phong c√°ch tr·∫£ l·ªùi: NG·∫ÆN G·ªåN + D·ªÑ ƒê·ªåC + C√≥ quy·∫øt ƒë·ªãnh ƒë·ªß/thi·∫øu th√¥ng tin.
    Tr·∫£ v·ªÅ JSON ƒë·ªÉ d·ªÖ parse.
    """
    return (
        "B·∫°n l√† tr·ª£ l√Ω h·ªçc thu·∫≠t. Ch·ªâ s·ª≠ d·ª•ng TH√îNG TIN TRONG CONTEXT.\n"
        "- N·∫øu ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi: t·∫°o c√¢u tr·∫£ l·ªùi NG·∫ÆN G·ªåN, D·ªÑ ƒê·ªåC, c√≥ ƒë√°nh d·∫•u tr√≠ch d·∫´n d·∫°ng [cit:1], [cit:2] theo c√°c m·ª•c trong CONTEXT.\n"
        "- N·∫øu KH√îNG ƒë·ªß th√¥ng tin: KH√îNG ƒëo√°n. Tr·∫£ tr·∫°ng th√°i NEED_MORE.\n"
        "ƒê·ªäNH D·∫†NG TR·∫¢ V·ªÄ: JSON duy nh·∫•t, theo schema:\n"
        "{\n"
        '  "status": "ANSWER" | "NEED_MORE",\n'
        '  "answer": "string (ch·ªâ khi status=ANSWER, ng·∫Øn g·ªçn, c√≥ [cit:N])",\n'
        '  "confidence": 0.0_to_1.0,\n'
        '  "reason": "string (gi·∫£i th√≠ch ng·∫Øn v√¨ sao thi·∫øu/ƒë·ªß)",\n'
        '  "suggested_top_k": 0  // khi NEED_MORE, g·ª£i √Ω top_k m·ªõi (l·ªõn h∆°n hi·ªán t·∫°i)\n'
        "}\n"
        "Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá, kh√¥ng k√®m vƒÉn b·∫£n n√†o kh√°c."
    )

import re

_cit_pat = re.compile(r"\[cit:(\d+)\]")

def extract_citation_ids(answer: str) -> list[int]:
    return [int(m.group(1)) for m in _cit_pat.finditer(answer)]

import re

import re

def strip_inline_citations(text: str) -> str:
    # Xo√° m·ªçi block [cit:1] ho·∫∑c [cit:1, cit:2, ...]
    text = re.sub(r"\[(?:\s*cit:\s*\d+\s*(?:,\s*)?)+\]", "", text, flags=re.IGNORECASE)
    return _tidy_punctuation(text)

def _tidy_punctuation(s: str) -> str:
    # Xo√° kho·∫£ng tr·∫Øng tr∆∞·ªõc d·∫•u , . ; :
    s = re.sub(r"\s+,", ",", s)
    s = re.sub(r"\s+\.", ".", s)
    s = re.sub(r"\s+;", ";", s)
    s = re.sub(r"\s+:", ":", s)

    # X·ª≠ l√Ω c√°c c·ª•m th·ª´a sau khi g·ª° cit
    s = re.sub(r",\s*\.", ".", s)     # ", ." -> "."
    s = re.sub(r"\s*,\s*,", ", ", s)  # ", ," -> ", "
    s = re.sub(r"\(\s+\)", "", s)     # "(   )" -> ""
    s = re.sub(r"\s{2,}", " ", s)     # nhi·ªÅu space -> 1 space
    s = re.sub(r"\n\s+\n", "\n\n", s) # d·ªçn kho·∫£ng tr·∫Øng th·ª´a gi·ªØa d√≤ng

    return s.strip()


def format_references(ids: list[int], hits: list, llm_answer: str = "", max_refs: int = 6):
    """
    ids: danh s√°ch [cit:N]; √°nh x·∫° sang hits[N-1]
    hits: c√≥ th·ªÉ l√† List[Dict] ho·∫∑c List[List[Dict]]
    llm_answer: c√¢u tr·∫£ l·ªùi t·ª´ model (c√≥ th·ªÉ ch·ª©a [cit:N])
    Tr·∫£ v·ªÅ tuple: (answer_cleaned, refs_block)
    """

    # --- 1. Flatten hits n·∫øu c√≥ list l·ªìng nhau ---
    flat_hits = []
    for h in hits:
        if isinstance(h, list):
            flat_hits.extend(h)
        elif isinstance(h, dict):
            flat_hits.append(h)
    hits = flat_hits

    # --- 2. T·∫°o danh s√°ch ngu·ªìn tham kh·∫£o ---
    seen = set()
    refs = []
    for n in ids:
        if n < 1 or n > len(hits):
            continue
        if n in seen:
            continue
        seen.add(n)
        h = hits[n - 1]
        title = (h.get("title") or "").strip() or "T√†i li·ªáu"
        pfrom, pto = h.get("page_from"), h.get("page_to")

        if pfrom is not None and pto is not None:
            refs.append(f"- {title} ‚Äî trang {pfrom}‚Äì{pto}")
        elif pfrom is not None:
            refs.append(f"- {title} ‚Äî trang {pfrom}")
        else:
            refs.append(f"- {title}")

        if len(refs) >= max_refs:
            break

    # --- 3. Fallback n·∫øu kh√¥ng c√≥ [cit:N] ---
    if not refs and hits:
        for i, h in enumerate(hits[: min(3, len(hits))], 1):
            title = (h.get("title") or "").strip() or "T√†i li·ªáu"
            pfrom, pto = h.get("page_from"), h.get("page_to")
            if pfrom is not None and pto is not None:
                refs.append(f"- {title} ‚Äî trang {pfrom}‚Äì{pto}")
            elif pfrom is not None:
                refs.append(f"- {title} ‚Äî trang {pfrom}")
            else:
                refs.append(f"- {title}")

    # --- 4. Xo√° to√†n b·ªô [cit:N] kh·ªèi c√¢u tr·∫£ l·ªùi LLM ---
    answer_cleaned = re.sub(r"\[cit:\d+\]", "", llm_answer).strip()

    # --- 5. T·∫°o block hi·ªÉn th·ªã ---
    refs_block = "\n".join(refs)
    if refs_block:
        refs_block = f"\n\nüìö **Ngu·ªìn tham kh·∫£o:**\n{refs_block}"

    return answer_cleaned, refs_block


def build_qa_prompt(query_text: str, hits: List[Dict[str, Any]]) -> str:
    ctx = _format_context_for_prompt(hits)
    instruction = _build_qa_instruction()
    return (
        f"{instruction}\n\n"
        f"USER QUESTION:\n{query_text}\n\n"
        f"CONTEXT (ngu·ªìn ƒë√£ tr√≠ch l·ª•c, c√≥ [#cit=N]):\n{ctx}\n"
    )

def _call_gemini_json(prompt: str) -> Dict[str, Any]:
    """
    G·ªçi Gemini, c·ªë g·∫Øng bu·ªôc JSON. C√≥ fallback model n·∫øu model ch√≠nh kh√¥ng kh·∫£ d·ª•ng.
    """
    models_try = [MODEL_QA]
    last_err = None
    for m in models_try:
        try:
            model = genai.GenerativeModel(m)
            # M·ªôt s·ªë SDK h·ªó tr·ª£ response_mime_type, nh∆∞ng ƒë·ªÉ an to√†n ta v·∫´n r√†ng bu·ªôc b·∫±ng prompt
            resp = model.generate_content(
                prompt,
                generation_config={
                    "temperature": 0.2
                }
            )
            text = resp.text if hasattr(resp, "text") else (resp.candidates[0].content.parts[0].text if resp.candidates else "")
            # Ch·ªâ nh·∫≠n JSON
            text = text.strip()
            # C·∫Øt guard n·∫øu model l·ª° th√™m code fence
            if text.startswith("```"):
                text = text.strip("`")
                # c√≥ th·ªÉ c√≤n 'json\n{...}'
                if "\n" in text:
                    text = text.split("\n", 1)[1]
            return json.loads(text)
        except Exception as e:
            last_err = e
            continue
    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu l·ªói, tr·∫£ NEED_MORE m·∫∑c ƒë·ªãnh
    return {
        "status": "NEED_MORE",
        "answer": "",
        "confidence": 0.0,
        "reason": f"LLM error: {last_err}",
        "suggested_top_k": 0
    }

def generate_answer_or_retry(
    query_text: str,
    hits: List[Dict[str, Any]],
    current_top_k: int,
    max_top_k_cap: int = 50
) -> Dict[str, Any]:
    """
    Tr·∫£ v·ªÅ 1 trong 2 nh√°nh:
    - {"status":"ANSWER","answer": "...","citations":[...], "confidence": float}
    - {"status":"RETRY","next_top_k": int, "reason": "..."}
    """
    # Heuristic nhanh: n·∫øu context qu√° √≠t -> y√™u c·∫ßu tƒÉng top_k tr∆∞·ªõc khi g·ªçi LLM
    if len(hits) == 0:
        next_k = min(max(current_top_k * 2, current_top_k + 5), max_top_k_cap)
        return {"status": "RETRY", "next_top_k": next_k, "reason": "No retrieval hits."}

    prompt = build_qa_prompt(query_text, hits)
    out = _call_gemini_json(prompt)
    if not isinstance(out, dict) or "status" not in out:
        # Kh√¥ng parse ƒë∆∞·ª£c ‚Üí th·ª≠ tƒÉng top_k
        next_k = min(max(current_top_k * 2, current_top_k + 5), max_top_k_cap)
        return {"status": "RETRY", "next_top_k": next_k, "reason": "LLM did not return valid JSON."}

    if out.get("status", "").upper() == "ANSWER":
        raw_answer = (out.get("answer") or "").strip()

        # 1) ids t·ª´ n·ªôi dung g·ªëc
        ids = extract_citation_ids(raw_answer)

        # 2) g·ª° [cit:N] + d·ªçn d·∫•u c√¢u
        clean_answer = strip_inline_citations(raw_answer)

        # 3) refs (string, KH√îNG ph·∫£i tuple)
        refs_block = format_references(ids, hits)  # <- tr·∫£ v·ªÅ string

        final_text = clean_answer
        if refs_block:
            final_text = f"{final_text}\n\nNgu·ªìn tham kh·∫£o:\n{refs_block}"

        return {
            "status": "ANSWER",
            "answer": final_text,
            "confidence": float(out.get("confidence") or 0.0),
        }

    # NEED_MORE ‚Üí ƒë·ªÅ ngh·ªã tƒÉng top_k
    suggested = int(out.get("suggested_top_k") or 0)
    if suggested <= current_top_k:
        suggested = current_top_k * 2
    next_k = min(suggested, max_top_k_cap)

    return {
        "status": "RETRY",
        "next_top_k": next_k,
        "reason": out.get("reason") or "Model signaled insufficient context."
    }

# ====== Pipeline ch·∫°y h·ªèi-ƒë√°p c√≥ v√≤ng l·∫∑p tƒÉng top_k ======
def run_qa_pipeline(
    query_text: str,
    keywords: Optional[Union[str, Iterable[str]]] = None,
    filters: Optional[dict] = None,
    initial_top_k: int = 5,
    max_top_k_cap: int = 50,
    max_iters: int = 4,
):
    """
    V√≤ng l·∫∑p:
      1) hybrid_search(top_k=current_k)
      2) generate_answer_or_retry(...)
         - N·∫øu ANSWER: in v√† tho√°t
         - N·∫øu RETRY: l·∫•y next_top_k (ƒë√£ k·∫πp max_top_k_cap), tƒÉng current_k v√† l·∫∑p
    """
    current_k = max(1, int(initial_top_k))
    for it in range(1, max_iters + 1):
        # TƒÉng rrf_window v√† num_candidates t∆∞∆°ng ·ª©ng ƒë·ªÉ RRF c√≥ "ƒë·∫•t" l·ª±a ch·ªçn
        rrf_window = max(50, min(max_top_k_cap * 5, current_k * 5))
        knn_num_candidates = max(200, min(2000, current_k * 10))
        
        # 1) L·∫•y hits ri√™ng cho t·ª´ng keyword
        hits_per_kw: List[List[Dict[str, Any]]] = []
        for kw in keywords:
            hits_kw = hybrid_search(
                query_text=query_text,
                keywords=kw,              # CH·ªà 1 keyword m·ªói l·∫ßn
                filters=filters,
                top_k=current_k,          # c√¥ng b·∫±ng m·ªói keyword l·∫•y c√πng top_k
                rrf_window=rrf_window,
                knn_num_candidates=knn_num_candidates,
                rrf_k=RRF_K,
            )
            hits_per_kw.append(hits_kw or [])
        # Kh√¥ng c√≥ k·∫øt qu·∫£ truy xu·∫•t
        if not hits_per_kw:
            next_k = min(max_top_k_cap, max(current_k * 2, current_k + 5))
            if next_k == current_k:
                return {
                    "status": "INSUFFICIENT_RETRIEVAL",
                    "reason": "Kh√¥ng c√≥ t√†i li·ªáu truy xu·∫•t v√† kh√¥ng th·ªÉ tƒÉng top_k th√™m.",
                    "last_top_k": current_k,
                    "iters": it,
                }
            current_k = next_k
            continue

        result = generate_answer_or_retry(
            query_text=query_text,
            hits=hits_per_kw,
            current_top_k=current_k,
            max_top_k_cap=max_top_k_cap
        )

        if result.get("status") == "ANSWER":
            return {
                "status": "ANSWER",
                "answer": result.get("answer", "").strip(),
                "confidence": result.get("confidence", 0.0),
                "last_top_k": current_k,
                "iters": it,
            }

       # ‚ùå Ch∆∞a ƒë·ªß th√¥ng tin ‚Üí retry
        next_k = int(result.get("next_top_k") or 0)
        reason = result.get("reason") or "Model signaled insufficient context."
        if next_k <= current_k:
            next_k = min(max_top_k_cap, max(current_k * 2, current_k + 5))

        if next_k == current_k:
            return {
                "status": "INSUFFICIENT_RETRIEVAL",
                "reason": reason,
                "last_top_k": current_k,
                "iters": it,
            }
        current_k = next_k
        
        
    # H·∫øt v√≤ng l·∫∑p m√† v·∫´n ch∆∞a c√≥ ANSWER
    return {
        "status": "INSUFFICIENT_RETRIEVAL",
        "reason": "ƒê√£ h·∫øt v√≤ng l·∫∑p m√† ch∆∞a ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi.",
        "last_top_k": current_k,
        "iters": max_iters,
    }


# ====== Entry point ======
if __name__ == "__main__":
    # V√≠ d·ª• ch·∫°y:
    print(run_qa_pipeline(
        query_text="ƒëi·ªÅu ki·ªán x√©t tuy·ªÉn th·∫≥ng",
        keywords=['ƒëi·ªÅu ki·ªán x√©t tuy·ªÉn th·∫≥ng', 'x√©t tuy·ªÉn th·∫≥ng'],
        filters=None,  # ho·∫∑c None ƒë·ªÉ t√¨m to√†n b·ªô
        initial_top_k=5,
        max_top_k_cap=20,
        max_iters=1
    ))
