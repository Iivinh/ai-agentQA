import os, re, json, time, random, argparse
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

import pdfplumber
from tqdm import tqdm
from dotenv import load_dotenv
from elasticsearch import Elasticsearch, helpers, exceptions
import google.generativeai as genai

# ===================== ENV & CLIENTS =====================
load_dotenv()

def get_es(es_url: Optional[str] = None) -> Elasticsearch:
    es_url = es_url or os.getenv("ES_URL", "http://localhost:9200")
    return Elasticsearch(es_url)

def configure_gemini():
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p trong .env")
    genai.configure(api_key=api_key, transport="rest")
    
    
# === K·∫æT N·ªêI: Elasticsearch & Gemini ===
def check_connections():
    es_url   = os.getenv("ES_URL", "http://localhost:9200")
    es_index = os.getenv("ES_INDEX", "school_knowledge")
    embed_model = os.getenv("EMBED_MODEL", "models/gemini-embedding-001")
    seg_model   = os.getenv("GEMINI_SEGMENT_MODEL", "gemini-2.5-flash")

    print("üîå Ki·ªÉm tra Elasticsearch‚Ä¶")
    try:
        es = get_es(es_url)
        ok = es.ping()
        info = es.info() if ok else {}
        print(f"  ‚Ä¢ ES_URL      : {es_url}")
        print(f"  ‚Ä¢ Ping        : {'OK' if ok else 'FAIL'}")
        if ok:
            print(f"  ‚Ä¢ Cluster     : {info.get('cluster_name')}")
            print(f"  ‚Ä¢ ES version  : {info.get('version', {}).get('number')}")
            # Th·ª≠ g·ªçi cat.indices (an to√†n n·∫øu kh√¥ng c√≥ quy·ªÅn, s·∫Ω b·∫Øt l·ªói)
            try:
                exists = es.indices.exists(index=es_index)
                print(f"  ‚Ä¢ Index '{es_index}': {'t·ªìn t·∫°i' if exists else 'ch∆∞a c√≥'}")
            except Exception as e:
                print(f"  ‚Ä¢ Kh√¥ng ki·ªÉm tra ƒë∆∞·ª£c index: {e}")
        else:
            print("‚ö†Ô∏è  Kh√¥ng ping ƒë∆∞·ª£c Elasticsearch.")
    except Exception as e:
        print(f"‚ùå L·ªói Elasticsearch: {e}")

    print("\nü§ñ Ki·ªÉm tra Gemini‚Ä¶")
    try:
        configure_gemini()
        # 1) th·ª≠ embed m·ªôt chu·ªói ng·∫Øn
        r = genai.embed_content(model=embed_model, content="health check")
        dims = len(r.get("embedding", []))
        print(f"  ‚Ä¢ EMBED_MODEL : {embed_model}")
        print(f"  ‚Ä¢ Embed dims  : {dims if dims else 'UNKNOWN'}")

        # 2) th·ª≠ generate_content r·∫•t ng·∫Øn (model ph√¢n ƒëo·∫°n)
        model = genai.GenerativeModel(seg_model)
        resp = model.generate_content("Ch·ªâ tr·∫£ l·ªùi: OK")
        txt = (resp.text or "").strip()
        print(f"  ‚Ä¢ SEGMENT_MODEL: {seg_model}")
        print(f"  ‚Ä¢ Gen sample   : {txt[:60]}")
        print("‚úÖ Gemini OK")
    except Exception as e:
        print(f"‚ùå L·ªói Gemini: {e}")

def clean_pdf_text(text: str) -> str:
    """Chu·∫©n ho√° text PDF, gi·ªØ xu·ªëng d√≤ng ƒëo·∫°n (\n\n), tr√°nh ƒë·ª©t t·ª´."""
    text = text or ""
    text = re.sub(r'(\w)-\s*\n(\w)', r'\1\2', text, flags=re.UNICODE)  # n·ªëi t·ª´ b·ªã g·∫°ch n·ªëi qua d√≤ng
    text = text.replace('\r', '')
    # gi·ªØ \n\n l√†m ng·∫Øt ƒëo·∫°n, c√≤n \n ƒë∆°n -> space
    placeholder = "<<<PARA>>>"
    text = text.replace("\n\n", placeholder)
    text = re.sub(r'[ \t]*\n[ \t]*', ' ', text)  # \n ƒë∆°n -> space
    text = text.replace(placeholder, "\n\n")
    text = re.sub(r'[ \t]+', ' ', text)  # chu·∫©n ho√° kho·∫£ng tr·∫Øng
    return text.strip()

def extract_pdf_pages(pdf_path: str, min_len: int = 20) -> List[Dict[str, Any]]:
    """ƒê·ªçc t·ª´ng trang v√† l√†m s·∫°ch. B·ªè trang qu√° ng·∫Øn."""
    pages: List[Dict[str, Any]] = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for i, p in enumerate(pdf.pages, start=1):
                raw = p.extract_text() or ""
                txt = clean_pdf_text(raw)
                if txt and len(txt) >= min_len:
                    pages.append({"page_num": i, "text": txt})
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói ƒë·ªçc {pdf_path}: {e}")
    return pages

def read_and_clean_pdfs(save: Optional[bool] = None, min_len: int = 20) -> Dict[str, List[Dict[str, Any]]]:
    """
    ƒê·ªçc to√†n b·ªô PDF trong PDF_DIR v√† l√†m s·∫°ch theo trang.
    - M·∫∑c ƒë·ªãnh KH√îNG l∆∞u ra file. Ch·ªâ l∆∞u khi save=True ho·∫∑c env SAVE_CLEAN=1.
    - Tr·∫£ v·ªÅ dict: {doc_id: [ {page_num, text}, ... ], ...}
    """
    # quy·∫øt ƒë·ªãnh l∆∞u hay kh√¥ng
    if save is None:
        save = os.getenv("SAVE_CLEAN", "0") == "1"

    PDF_DIR = os.getenv("PDF_DIR", "data/pdf")
    OUT_DIR = "clean"

    if not os.path.isdir(PDF_DIR):
        print(f"‚ö†Ô∏è Th∆∞ m·ª•c PDF kh√¥ng t·ªìn t·∫°i: {PDF_DIR}")
        return {}

    pdfs = [os.path.join(PDF_DIR, f) for f in os.listdir(PDF_DIR) if f.lower().endswith(".pdf")]
    print(f"üìÇ T√¨m th·∫•y {len(pdfs)} file PDF trong {PDF_DIR}")

    results: Dict[str, List[Dict[str, Any]]] = {}
    for pdf_path in pdfs:
        base = os.path.basename(pdf_path)
        doc_id = os.path.splitext(base)[0]
        pages = extract_pdf_pages(pdf_path, min_len=min_len)
        if not pages:
            print(f"‚ö†Ô∏è B·ªè qua (kh√¥ng c√≥ text h·ª£p l·ªá): {base}")
            continue

        results[doc_id] = pages
        print(f"‚úÖ {base}: {len(pages)} trang h·ª£p l·ªá")

        if save:
            os.makedirs(OUT_DIR, exist_ok=True)
            # l∆∞u JSONL theo trang
            jsonl_path = os.path.join(OUT_DIR, f"{doc_id}.pages.jsonl")
            with open(jsonl_path, "w", encoding="utf-8") as f:
                for rec in pages:
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n")
            # l∆∞u full TXT g·ªôp trang
            fulltxt_path = os.path.join(OUT_DIR, f"{doc_id}.full.txt")
            with open(fulltxt_path, "w", encoding="utf-8") as f:
                for rec in pages:
                    f.write(f"=== Page {rec['page_num']} ===\n{rec['text']}\n\n")
            print(f"   ‚Ü≥ ƒê√£ l∆∞u: {jsonl_path}, {fulltxt_path}")

    return results

def group_pages_by_window(
    pages: List[Dict[str, Any]],
    block_size: int = 3,
    overlap: int = 1
) -> List[Dict[str, Any]]:
    """Gom N trang th√†nh 1 block, tr∆∞·ª£t overlap."""
    blocks = []
    n = len(pages)
    i = 0
    step = block_size - overlap
    while i < n:
        j = min(i + block_size, n)
        block_pages = pages[i:j]
        if not block_pages:
            break
        text = "\n\n".join(p["text"] for p in block_pages if p["text"])
        blocks.append({
            "page_from": block_pages[0]["page_num"],
            "page_to":   block_pages[-1]["page_num"],
            "text": text.strip()
        })
        if j == n:
            break
        i += step
    return blocks

def _strip_code_fences(s: str) -> str:
    # B·ªè ```json ... ``` ho·∫∑c ``` ... ```
    s = s.strip()
    if s.startswith("```"):
        s = re.sub(r"^```(?:json|JSON)?\s*", "", s)
        s = re.sub(r"\s*```$", "", s)
    return s.strip()

def _largest_json_object(s: str) -> Optional[str]:
    # T√¨m kh·ªëi JSON { ... } l·ªõn nh·∫•t (c√¢n ngo·∫∑c)
    start = None; depth = 0; best = None
    for i, ch in enumerate(s):
        if ch == '{':
            if depth == 0: start = i
            depth += 1
        elif ch == '}':
            if depth > 0:
                depth -= 1
                if depth == 0 and start is not None:
                    cand = s[start:i+1]
                    if not best or len(cand) > len(best):
                        best = cand
    return best

def _safe_json_loads(s: str) -> Optional[dict]:
    if not s: return None
    s = _strip_code_fences(s)
    # Th·ª≠ parse tr·ª±c ti·∫øp
    try:
        return json.loads(s)
    except Exception:
        pass
    # Th·ª≠ t√¨m kh·ªëi { ... } l·ªõn nh·∫•t
    blob = _largest_json_object(s)
    if blob:
        try:
            return json.loads(blob)
        except Exception:
            return None
    return None

def _sanitize_for_log(s: str) -> str:
    # b·ªè control chars ƒë·ªÉ log an to√†n
    return ''.join(ch for ch in s if ch == '\n' or 32 <= ord(ch) <= 126 or ord(ch) >= 160).strip()


def segment_with_gemini(raw_block: str, model_name: Optional[str] = None,
                        max_retries: int = 3, save_raw: bool = False,
                        raw_path: Optional[str] = None) -> List[Dict[str, str]]:
    """
    G·ªçi Gemini ƒë·ªÉ c·∫Øt block th√†nh c√°c ƒëo·∫°n m·∫°ch l·∫°c.
    N·∫øu kh√¥ng parse ƒë∆∞·ª£c JSON, fallback: tr·∫£ 1 segment ch·ª©a to√†n b·ªô raw_block.
    Khi save_raw=True, ghi nguy√™n response ra file ƒë·ªÉ ki·ªÉm tra.
    """
    model_name = model_name or os.getenv("GEMINI_SEGMENT_MODEL", "gemini-2.5-flash")

    system_prompt = (
        "B·∫°n l√† c√¥ng c·ª• ph√¢n ƒëo·∫°n vƒÉn b·∫£n h·ªçc thu·∫≠t/quy ch·∫ø.\n"
        "Chia vƒÉn b·∫£n sau th√†nh c√°c ƒëo·∫°n m·∫°ch l·∫°c theo √Ω nghƒ©a.\n"
        "- Kh√¥ng c·∫Øt gi·ªØa c√¢u; gi·ªØ ƒë√∫ng th·ª© t·ª±; kh√¥ng b·ªè s√≥t.\n"
        "Ch·ªâ tr·∫£ v·ªÅ JSON h·ª£p l·ªá d·∫°ng:\n"
        "{\"segments\":[{\"title\":\"\",\"text\":\"...\"}, ...]}\n"
    )
    user_input = f"VƒÉn b·∫£n:\n{raw_block}"

    model = genai.GenerativeModel(model_name)

    last_txt = ""
    for attempt in range(max_retries):
        try:
            resp = model.generate_content([system_prompt, user_input])
            txt = (resp.text or "").strip()
            last_txt = txt
            payload = _safe_json_loads(txt)
            if not payload:
                raise ValueError("LLM kh√¥ng tr·∫£ JSON h·ª£p l·ªá")

            segments = payload.get("segments", [])
            out: List[Dict[str, str]] = []
            for seg in segments:
                title = (seg.get("title") or "").strip()
                text  = (seg.get("text")  or "").strip()
                if text:
                    out.append({"title": title, "text": text})
            if out:
                # l∆∞u raw n·∫øu c·∫ßn
                if save_raw and raw_path:
                    os.makedirs(os.path.dirname(raw_path), exist_ok=True)
                    with open(raw_path, "w", encoding="utf-8") as f:
                        f.write(_sanitize_for_log(txt))
                return out
            else:
                raise ValueError("JSON h·ª£p l·ªá nh∆∞ng kh√¥ng c√≥ segments")
        except Exception as e:
            if attempt == max_retries - 1:
                # Fallback: 1 segment = to√†n block
                print(f"‚ùå Gemini segmentation l·ªói: {e} ‚Üí d√πng fallback 1 segment.")
                if save_raw and raw_path:
                    try:
                        os.makedirs(os.path.dirname(raw_path), exist_ok=True)
                        with open(raw_path, "w", encoding="utf-8") as f:
                            f.write(_sanitize_for_log(last_txt))
                    except Exception:
                        pass
                return [{"title": "", "text": raw_block}]
            time.sleep(2 ** attempt)



def segment_docs(
    pages_by_doc: Dict[str, List[Dict[str, Any]]],
    block_size: int = 3,
    overlap: int = 1,
    save: Optional[bool] = None
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Gom trang -> block -> segment b·∫±ng Gemini.
    Tr·∫£ v·ªÅ dict {doc_id: [segment,...]}.
    N·∫øu save=True ho·∫∑c SAVE_SEGMENT=1 th√¨ l∆∞u ra segments/<doc_id>.jsonl
    """
    if save is None:
        save = os.getenv("SAVE_SEGMENT", "0") == "1"

    out_all: Dict[str, List[Dict[str, Any]]] = {}
    os.makedirs("segments", exist_ok=True)

    for doc_id, pages in pages_by_doc.items():
        print(f"üìë {doc_id}: {len(pages)} trang ‚Üí gom {block_size} trang 1 block (overlap={overlap})")
        blocks = group_pages_by_window(pages, block_size=block_size, overlap=overlap)
        segments: List[Dict[str, Any]] = []

        for idx, b in enumerate(blocks, start=1):
            rawfile = None
            want_save = save or (os.getenv("SAVE_SEGMENT", "0") == "1")
            if want_save:
                rawfile = os.path.join("segments", f"{doc_id}_block_{b['page_from']}-{b['page_to']}.raw.txt")
            segs = segment_with_gemini(
                b["text"],
                save_raw=want_save,
                raw_path=rawfile
            )
            for s in segs:
                s["page_from"] = b["page_from"]
                s["page_to"]   = b["page_to"]
            segments.extend(segs)


        out_all[doc_id] = segments
        print(f"   ‚Üí {len(blocks)} blocks, Gemini tr·∫£ {len(segments)} segments")

        if save and segments:
            seg_path = os.path.join("segments", f"{doc_id}.jsonl")
            with open(seg_path, "w", encoding="utf-8") as f:
                for seg in segments:
                    f.write(json.dumps(seg, ensure_ascii=False) + "\n")
            print(f"   ‚Ü≥ ƒê√£ l∆∞u {seg_path}")

    return out_all

def ensure_index(es: Elasticsearch, index_name: str, dims: int) -> None:
    try:
        if es.indices.exists(index=index_name):
            return
    except Exception:
        pass

    body = {
        "settings": {
            "number_of_shards": 1,
            "number_of_replicas": 0
        },
        "mappings": {
            "properties": {
                "doc_id":      {"type": "keyword"},
                "title":       {"type": "text"},
                "text":        {"type": "text"},
                "page_from":   {"type": "integer"},
                "page_to":     {"type": "integer"},
                "ingested_at": {"type": "date"},
                # ES 8/9: dense_vector native kNN (kh√¥ng c·∫ßn plugin)
                "vector": {
                    "type": "dense_vector",
                    "dims": dims,
                    "index": True,
                    "similarity": "cosine"
                }
            }
        }
    }
    es.indices.create(index=index_name, body=body)
    print(f"üÜï ƒê√£ t·∫°o index '{index_name}' (dims={dims})")
    
    
def embed_text(text: str, model_name: Optional[str] = None, max_retries: int = 4) -> List[float]:
    model_name = model_name or os.getenv("EMBED_MODEL", "models/gemini-embedding-001")
    for attempt in range(max_retries):
        try:
            r = genai.embed_content(
                model=model_name,
                content=text,
                task_type="retrieval_document"
            )
            vec = r.get("embedding", [])
            if not vec:
                raise RuntimeError("Empty embedding")
            return vec
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep((2 ** attempt) + random.uniform(0, 0.4))
    return []

    
def iter_actions_for_bulk(
    segments_by_doc: Dict[str, List[Dict[str, Any]]],
    embed_model: Optional[str] = None
):
    embed_model = embed_model or os.getenv("EMBED_MODEL", "models/gemini-embedding-001")
    for doc_id, segs in segments_by_doc.items():
        for i, s in enumerate(segs):
            title = (s.get("title") or "").strip()
            text  = (s.get("text")  or "").strip()
            if not text:
                continue
            # Embed
            vec = embed_text(text, model_name=embed_model)
            # T·∫°o action
            _id = f"{doc_id}-{s.get('page_from','')}-{s.get('page_to','')}-{i}"
            yield {
                "_op_type": "index",
                "_id": _id,
                "doc_id": doc_id,
                "title": title,
                "text": text,
                "page_from": int(s.get("page_from") or 0),
                "page_to": int(s.get("page_to") or 0),
                "ingested_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
                "vector": vec
            }



def embed_and_index(segments_by_doc: Dict[str, List[Dict[str, Any]]]) -> Tuple[int,int]:
    es_url   = os.getenv("ES_URL", "http://localhost:9200")
    es_index = os.getenv("ES_INDEX", "school_knowledge")
    embed_model = os.getenv("EMBED_MODEL", "models/gemini-embedding-001")

    configure_gemini()
    es = get_es(es_url)

    # L·∫•y dims 1 l·∫ßn ƒë·ªÉ t·∫°o mapping ch√≠nh x√°c
    probe = genai.embed_content(model=embed_model, content="probe")
    dims = len(probe.get("embedding", [])) or 3072  # fallback 768 n·∫øu API kh√¥ng tr·∫£

    ensure_index(es, es_index, dims)

    print("üöÄ B·∫Øt ƒë·∫ßu embed + index ‚Ä¶")
    success, fail = 0, 0
    # G·ªçi bulk theo batch gi√∫p ·ªïn ƒë·ªãnh b·ªô nh·ªõ
    batch, BATCH_SIZE = [], 200
    for act in iter_actions_for_bulk(segments_by_doc, embed_model=embed_model):
        # g·∫Øn index v√†o action
        act["_index"] = es_index
        batch.append(act)
        if len(batch) >= BATCH_SIZE:
            ok, err = helpers.bulk(es, batch, raise_on_error=False)
            success += ok; fail += len(err)
            batch.clear()
    if batch:
        ok, err = helpers.bulk(es, batch, raise_on_error=False)
        success += ok; fail += len(err)

    print(f"‚úÖ Indexed: {success} | ‚ùå Errors: {fail}")
    return success, fail


if __name__ == "__main__":
    check_connections()
    pages_by_doc = read_and_clean_pdfs()
    segments_by_doc = segment_docs(pages_by_doc, block_size=5, overlap=1, save=True)
    embed_and_index(segments_by_doc)